---
title: "metapsyTools"
output: rmarkdown::html_vignette
author: "Mathias Harrer, Paula Kuper & Antonia Sprenger"
vignette: >
  %\VignetteIndexEntry{metapsyTools}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

<br></br>

## Introduction

------------------------------------------------------------------------

The `metapsyTools` package facilitates the calculation of effect sizes (i.e. Hedges' $g$) and meta-analyses using the [**Metapsy**](https://www.metapsy.org) databases (or databases adhering to the same format).

The package consists of **two modules**:

1.  A module to check the data format and calculate effect sizes for all possible study comparisons (**preparation** module);
2.  A module to select relevant comparisons for a specific meta-analysis, calculate the results (including subgroup, meta-regression, and publication bias analyses), and generate tables (**analysis** module).

The idea is to use the two modules in different contexts. For example, the **preparation** module can be used every time the database is updated to gather all information, calculate effect sizes, and bring the data into a format suitable for further analyses.

Prepared data sets that follow the [Metapsy data standard](https://docs.metapsy.org/data-preparation/format/) build the basis for the **analysis** module. Researchers simply have to filter out the comparisons that are relevant for the investigation, and can then use functions of the package to run a full meta-analysis, including sensitivity and subgroup analyses.

<br>

![](overview.png){width="685px"}

::: {style="border: 1px solid #dee2e6; background-color: #f1f3f5 !important; padding: 10px; border-radius: 3px;"}
<strong>Important</strong>: The preparation module requires a familiarity with the general structure of the database, as well as intermediate-level knowledge of R and the `metapsyTools` package itself, in order to diagnose (potential) issues. We advise that the preparation step (i.e. checking the data format and calculating effect sizes each time the database is updated) should be conducted by a person with some proficiency in R & data wrangling. The analysis module, on the other hand, should be usable by all researchers.
:::

<br></br>

<br></br>

## The Preparation Module

------------------------------------------------------------------------

The preparation module in `metapsyTools` allows to:

-   check if the classes of variables in the imported data set are as required, using `checkDataFormat`
-   check for formatting conflicts which have to be resolved prior to any further steps, using `checkConflicts`
-   calculate Hedges' $g$ and/or risk ratios ($RR$) for each comparison, provided suitable data is available, and generate a data set ready for meta-analyses, using `calculateEffectSizes`.

<br></br>

### Required Data Structure

For your own convenience, it is highly advised to **strictly** follow some data formatting rules prior to importing a data set for effect size calculations. If all of these rules are followed closely, it minimizes the risk of receiving error messages, producing conflicts, etc.

A detailed description of the preferred data structure can be found in the [Metapsy documentation](https://docs.metapsy.org/data-preparation/format/).

To use the data preparation module (i.e. calculate effect sizes), study design (I) and effect size data variables (II) must be included in the data set.

<br>

#### Study Design Variables (I)

-   **`study`**: The study name, formatted as "last name of the first author", "year" (e.g. `"Smith, 2011"`).
-   **`condition_arm1`**: Condition in the first trial arm. The condition name is standardized to ensure comparability across trials (e.g. `cbt` for all trial arms that employed cognitive-behavioral psychotherapy).
-   **`condition_arm2`**: Condition in the second trial arm. The condition name is standardized to ensure comparability across trials (e.g. `wlc` for all trial arms that employed a waitlist control group).
-   **`multi_arm1`**: In multiarm trials, this variable provides a "specification" of the type of treatment used in the first arm. This variable is set to `NA` (missing) when the study was not a multiarm trial. For example, if a multiarm trial employed two types of CBT interventions, face-to-face and Internet-based, this variable would be set to `f2f` and `Internet`, respectively.
-   **`multi_arm2`**: In multiarm trials, this variable provides a "specification" of the type of treatment used in the second arm. This variable is set to `NA` (missing) when the study was not a multiarm trial. For example, if a multiarm trial employed two types of control groups, waitlist and placebo, this variable would be set to `wl` and `plac`, respectively.
-   **`outcome_type`**: This variable encodes the type of outcome that builds the basis of the comparison, e.g. `response`, `remission` or `deterioration`. This is variable is particularly relevant for dichotomous effect size data, because it indicates what the event counts refer to. The `msd` factor level is used for outcomes expressed in means and standard deviations.
-   **`instrument`**: This variable describes the instrument through which the relevant outcome was measured.
-   **`time`**: The measurement point at which the outcome was obtained (e.g. `post` or `follow-up`).
-   **`time_weeks`**: The measurement point at which the outcome was obtained, in weeks after randomization (set to `NA` if this information was not available).
-   **`rating`**: This variable encodes if the reported outcome was self-reported (`"self-report"`) or clinician-rated (`"clinician"`).

<br>

#### Effect Size Data Variables (II)

Each Metapsy database also contains variables in which the (raw or pre-calculated) effect size data is stored. In each row, one of the following variable groups (a) to (d) will be specified, depending on the type of outcome data reported in the paper. The rest of the variable groups will contain `NA` in that row.

<br>

**(a)** Continuous Outcome Data

-   **`mean_arm1`**: Mean of the outcome in the first arm at the measured time point.
-   **`mean_arm2`**: Mean of the outcome in the second arm at the measured time point.
-   **`sd_arm1`**: Standard deviation of the outcome in the first arm at the measured time point.
-   **`sd_arm2`**: Standard deviation of the outcome in the second arm at the measured time point.
-   **`n_arm1`**: Sample size in the first trial arm.
-   **`n_arm2`**: Sample size in the second trial arm.

<br>

**(b)** Change Score Data

-   **`mean_change_arm1`**: Mean score change between baseline and the measured time point in the first arm.
-   **`mean_change_arm2`**: Mean score change between baseline and the measured time point in the second arm.
-   **`sd_change_arm1`**: Standard deviation of the mean change in the first arm.
-   **`sd_change_arm2`**: Standard deviation of the mean change in the second arm.
-   **`n_change_arm1`**: Sample size in the first trial arm.
-   **`n_change_arm2`**: Sample size in the second trial arm.

<br>

**(c)** Dichotomous Outcome Data (Response, Remission, Deterioration, ...)

-   **`event_arm1`**: Number of events (responders, remission, deterioration cases) in the first trial arm.
-   **`event_arm2`**: Number of events (responders, remission, deterioration cases) in the second trial arm.
-   **`totaln_arm1`**: Sample size in the first trial arm.
-   **`totaln_arm2`**: Sample size in the second trial arm.

<br>

**(d)** Pre-calculated Hedges' $g$

-   **`precalc_g`**: The pre-calculated value of Hedges' $g$ (small-sample bias corrected standardized mean difference; [Hedges, 1981](https://journals.sagepub.com/doi/10.3102/10769986006002107)).
-   **`precalc_g_se`**: Standard error of $g$, viz. $\sqrt{V_g}$.

<br>

**(d)** Pre-calculated log-risk ratio

-   **`precalc_log_rr`**: The pre-calculated value of the log-risk ratio $\log_{e}\text{RR}$, comparing events in the first arm to events in the second arm.
-   **`precalc_log_rr_se`**: The standard error of the log-risk ratio $\log_{e}\text{RR}$, comparing events in the first arm to events in the second arm.

<br></br>

#### Additional Considerations

Metapsy databases also contain additional variables. These are used, for example, to collect subject-specific information that is not relevant for all indications. Nevertheless, there are several formatting rules that all variables/columns follow:

-   All variable names are in [`snake_case`](https://en.wikipedia.org/wiki/Snake_case?oldformat=true).
-   Variable names start with a standard letter (`_` is not allowed, `.` is only allowed for `metapsyTools` variables).
-   Variable names do not contain special characters (like ö, \@, è, ğ).
-   Semicolons (`;`) are not used; neither as variable names nor as cell content.
-   Character values contain no leading/trailing whitespaces.
-   Missing values are encoded using `NA`.

<br></br>

### Data Format & Conflict Check

Once the data set has been pre-structured correctly, `metapsyTools` allows you to check the required variable format and control for potential formatting conflicts.

If the formatting rules above are followed, none of the default behavior of the preparation module has to be changed. At first, one can run `checkDataFormat`. As an illustration, we will use the `depressionPsyCtr` data frame, which is directly available after installing `metapsyTools`.

```{r, eval=F}
library(metapsyTools)
data("depressionPsyCtr")
depressionPsyCtr <- checkDataFormat(depressionPsyCtr)
```

    ## - [OK] Data set contains all variables in 'must.contain'.
    ## - [OK] 'study' has desired class character.
    ## - [OK] 'condition_arm1' has desired class character.
    ## - [OK] 'condition_arm2' has desired class character.
    ## - [OK] 'multi_arm1' has desired class character.
    ## - [OK] 'multi_arm2' has desired class character.
    ## - [OK] 'outcome_type' has desired class character.
    ## - [OK] 'instrument' has desired class character.
    ## - [OK] 'time' has desired class character.
    ## - [OK] 'time_weeks' has desired class numeric.
    ## - [OK] 'rating' has desired class character.


We see that the function has checked the required variables and their class. If divergent, the function will also try to **convert** the variable to the desired format.

You can avoid the prompt asking for the database format if you specify the `data.format` argument directly in the function:

::: {style="border: 1px solid #dee2e6; background-color: #f1f3f5 !important; padding: 10px; border-radius: 3px;"}
<strong>Required variables for unique IDs</strong>: To function properly, the `metapsyTools` functions must be able to generate a *unique* ID for each comparison. By default, this is achieved by combining information of the `study`, `condition_arm1`, `condition_arm2`, `multi_arm1`, `multi_arm2`, `outcome_type`, `instrument`, `time`, `time_weeks`, `rating` variables. These variables are included by default in the `must.contain` argument in the `checkDataFormat` function. If all required variables are detected, an `OK` message is returned; as is the case in our example.
:::

We can also check the data for potential formatting issues, using the `checkConflicts` function.

```{r, eval=F}
depressionPsyCtr <- checkConflicts(depressionPsyCtr) 
```

    ## - [OK] No data format conflicts detected.

In our case, no data format conflicts were detected. This is because all the data were pre-structured correctly. If data conflicts exist, the output looks similar to this:

    - [!] Data format conflicts detected!
    ID conflicts 
    - check if variable(s) study create(s) unique assessment point IDs 
    - check if specification uniquely identifies all trial arms in multiarm trials 
    --------------------
    Abas, 2018
    Ammerman, 2013
    Andersson, 2005
    Arean, 1993
    Ayen, 2004
    Barrett, 2001
    Bedard, 2014
    Bowman, 1995
    Brown, 1984

<br></br>

### Effect Size Calculation

After the multiarm expansion, the effect size (Hedges' $g$ or risk ratio) and its variance can be calculated for all comparisons, using `calculateEffectSizes`. When applying `calculateEffectSizes`, the effect size functions specified in the `funcs` argument are applied by default, which means that effect sizes based on Mean, SD and N, and binary outcomes can be calculated.

The `calculateEffectSizes` function adds nine columns to the data set, all of which start with a dot (`.`). They are included so that meta-analysis functions in `metapsyTools` can be applied "out of the box".

-   **`.id`**: Unique identifier for a trial arm comparison/row.
-   **`.g`**: Calculated effect size (Hedges' $g$).
-   **`.g_se`**: Standard error of Hedges' $g$.
-   **`.log_rr`**: Calculated effect size ($\log_{e}\text{RR}$).
-   **`.log_rr_se`**: Standard error of $\log_{e}\text{RR}$.
-   **`.event_arm1`**: Number of events (e.g. responders) in the first trial arm.
-   **`.event_arm2`**: Number of events (e.g. responders) in the second trial arm.
-   **`.totaln_arm1`**: Total sample size in the first trial arm.
-   **`.totaln_arm2`**: Total sample size in the second trial arm.

```{r, eval=F}
data <- calculateEffectSizes(depressionPsyCtr)
```

    - [OK] Hedges' g calculated successfully.
    - [OK] Log-risk ratios calculated successfully.

Please note that the current version of the data set only includes all unique trial arm **combinations**, not all *unique* **comparisons**. Let us a four-arm trial as an example. Say that this trial included CBT, PST, CAU and a wait-list condition. While this trial provides 6 unique trial arm combinations, the number of unique *comparisons* is higher: $\frac{n!}{(n-k)!} = \frac{4!}{2!} = 12$.

This is because the sign of the calculated effect size depends on which treatment serves as the *reference*. The effect of CBT vs. PST, for example, depends on which arm serves as the reference group (i.e. CBT-PST or PST-CBT). Depending on which arm is chosen (and assuming non-zero mean differences), the calculated effect size (e.g. Hedges' $g$) value will either be negative (e.g. $g$=-0.31) or positive ($g$=0.31). We see this visualized in the graph below. There are now two directed arrows for each comparison (12 in total), and each arrow reads as "is compared to":

<center>

![](graph2.png){width="200px"}

</center>

To truly calculate all unique *comparisons*, we have to set the `include.switched.arms` argument to `TRUE` when running `calculateEffectSizes`.

```{r, eval=F}
data <- calculateEffectSizes(depressionPsyCtr,
                             include.switched.arms = TRUE)

```

The resulting data frame `data` now includes all effect sizes that can theoretically be calculated. As a last step, one has to manually add the effect sizes and standard errors of comparisons for which `calculateEffectSizes` returned `NA`. This leads to a final data set which can be used with the **analysis module**, which will be described next.

::: {style="border: 1px solid #dee2e6; background-color: #f1f3f5 !important; padding: 10px; border-radius: 3px;"}
<strong>Should I use `include.switched.arms`?</strong> It is not essential to calculate all possible arm comparisons. However, having all possible comparisons in your data set is convenient, because all required comparisons pertaining to a particular research question (e.g. "how effective is PST when CBT is the reference group?") can directly be filtered out; there is typically no need for further data manipulation steps.
:::

<br></br>

<br></br>

## The Analysis Module

------------------------------------------------------------------------

The analysis module in `metapsyTools` allows to run different kinds of meta-analyses based on the final data set created by the preparation module. It is designed to also be usable by researchers who were not directly involved in the preparation step, and is less demanding in terms of required background knowledge.

Metapsy databases included in [`metapsyData`](https://data.metapsy.org) can directly be used by the analysis module in `metapsyTools`.

The analysis module allows, among other things, to:

-   Filter data using strict rules, fuzzy string matching, or by implementing prioritization rules (`filterPoolingData` and `filterPriorityRule`).
-   Run conventional (random-effects) meta-analysis models as well as commonly reported sensitivity analyses (`runMetaAnalysis`)
-   Correct effect estimates for potential publication bias or small-study effects (`correctPublicationBias`)
-   Run subgroup analyses based on the fitted model (`subgroupAnalysis`)
-   Create study information tables (`createStudyTable`).

<br>

### Filtering Relevant Comparisons

The `metapsyTools` package contains functions which allow to flexibly filter out comparisons that are relevant for a particular meta-analysis.

The `filterPoolingData` function is the main function for filtering. We simply have to provide the final data set with calculated effect sizes (see [The Preparation Module]), as well as one or several filtering criteria pertaining to our specific research question.

Say that we want to run a meta-analysis of all studies in which CBT was compared to wait-lists, with the BDI-II at post-test being the analyzed outcome. We can filter out the relevant comparisons like this:

```{r, eval=FALSE}
data %>% 
    filterPoolingData(condition_arm1 == "cbt",
                      condition_arm2 == "wl",
                      instrument == "bdi-1",
                      time == "post") %>% nrow()
```

    ## [1] 7

We see that $k$=7 comparisons fulfill these criteria. Note, however, that this will only select comparisons for which `condition_arm1` was *exactly* defined as `"cbt"`.

It is also possible to filter out all rows that simply *contain* a specific search term (**fuzzy matching**). For example, instead of `"cbt"`, we might want to filter out all studies that employed some type of "other" therapy format. This can be achieved by using the `Detect` function within `filterPoolingData`:

```{r, eval=F}
data %>% 
  filterPoolingData(Detect(condition_arm1, "other")) %>% 
  nrow()
```

    ## [1] 9

We see that this returns $k$=9 entries.

We can create even more complex filters. Say that we want to examine the effect, as measured by the BDI-I at post-test, of CBT, PST, and BAT compared to *either* CAU or wait-lists. We can visualize this with a graph again:

```{r, eval=F, echo=F}
library(igraph)
vertex = c("CBT", "CAU", "CBT", "WL", "PST", "CAU", "PST", "WL", "BAT", "CAU", "BAT", "WL")
g1 <- graph(edges=vertex, directed=TRUE) 
plot(g1, vertex.size = 35, vertex.color = "lightblue",
     vertex.label.family="Helvetica", layout=layout.circle)
```

<center>

![](graph3.png){width="200px"}

</center>

We can use the OR-Operator `|` within `filterPoolingData` for such filter types. Again, we use the `Detect` function to allow for fuzzy matching:

```{r, eval=F}
data %>% 
  filterPoolingData(Detect(condition_arm1, "cbt|pst|bat"),
                    Detect(condition_arm2, "cau|wl"),
                    instrument == "bdi-1",
                    time == "post") %>% nrow()
```

    ## [1] 11

<br>

Lastly, one can also filter data according to a specified **priority rule**, using the `filterPriorityRule` function. This is particularly helpful to select instruments. Say, for example, that we ordered certain instruments based on their known reliability. Now, for each study, we only want to select the comparison in which the most reliable instrument was used. It is possible that, for some studies, all used instruments are relatively unreliable. However, given our priority rule, we can still extract the comparison with a *relatively* high reliability, and discard all the other measurements within one study that are even less reliable.

Assume that our priority rule for the employed instrument is `"hdrs"` (priority 1), `"bdi-2"` (priority 2), `"phq-9"` (priority 3) and `"bdi-1"` (priority 4). We can implement the rule like this:

```{r, eval=F}
data %>% 
  # First, filter all other relevant characteristics
  filterPoolingData(Detect(condition_arm1, "cbt|pst|bat"),
                    Detect(condition_arm2, "cau|wl"),
                    time == "post") %>% 
  # Now, implement the priority rule for the outcome instrument
  filterPriorityRule(instrument = c("hdrs", "bdi-2", 
                                    "phq-9", "bdi-1")) %>% 
  # Show number of entries
  nrow()
```

    ## [1] 15

<br>

### Pooling Effects

After all relevant rows have been filtered out, we can start to pool the effect sizes. The `runMetaAnalysis` function serves as a wrapper for several commonly used meta-analytic approaches, and, by default, applies them all at once to our data:

-   `"overall"`. Runs a generic inverse-variance (random-effects) model. All included effect sizes are treated as independent.
-   `"combined"`. Pools all effect sizes within one study before calculating the overall effect. This ensures that all effect sizes are independent (i.e., unit-of-analysis error & double-counting is avoided). To combine the effects, one has to assume a **correlation** of effect sizes within studies, empirical estimates of which are typically not available. By default, `runMetaAnalysis` assumes that $\rho$=0.5.
-   `"lowest.highest"`. Runs a meta-analysis, but with only (i) the lowest and (ii) highest effect size within each study included.
-   `"outlier"`. Runs a meta-analysis without statistical outliers (i.e. effect sizes for which the confidence interval does not overlap with the confidence interval of the overall effect).
-   `"influence"`. Runs a meta-analysis without influential cases (see `influence.rma.uni` for details).
-   `"rob"`. Runs a meta-analysis with only low-RoB studies included. By default, only studies with a value `> 2` in the `rob` variable are considered for this analysis.
-   `"threelevel"`. Runs a multilevel (three-level) meta-analysis model, with effect sizes nested in studies.
-   `"threelevel.che"`. Runs a multilevel (three-level) "correlated and hierarchical effects" (CHE) meta-analysis model. In this model, effect sizes are nested in studies, and effects within studies are assumed to be correlated. This will typically be a plausible modeling assumption in most real-world use cases.

```{r, eval=F}
res <- runMetaAnalysis(ma.data)
```

    ## - Running meta-analyses...
    ## - [OK] Using Hedges' g as effect size metric... 
    ## - [OK] Calculating overall effect size... DONE
    ## - [OK] Calculating effect size using only lowest effect... DONE
    ## - [OK] Calculating effect size using only highest effect... DONE
    ## - [OK] Calculating effect size using combined effects (rho=0.5)... DONE
    ## - [OK] Calculating effect size with outliers removed... DONE
    ## - [OK] Calculating effect size with influential cases removed... DONE
    ## - [OK] Calculating effect size using only low RoB information... DONE
    ## - [OK] Calculating effect size using three-level MA model... DONE
    ## - [OK] Robust variance estimation (RVE) used for three-level MA model... DONE
    ## - [OK] Calculating effect size using three-level CHE model (rho=0.5)... DONE
    ## - [OK] Robust variance estimation (RVE) used for three-level CHE model... DONE
    ## - [OK] Done!

We can inspect the results by calling the created `res` object in the console. By default, an **HTML table** should pop up along with the console output. These pre-formatted HTML results tables can easily be transferred to, for example, MS Word using copy & paste.

```{r, eval=F}
res
```

    ## Model results ------------------------------------------------ 
    ##   model                       k     g g.ci           p         i2 i2.ci      prediction.ci   nnt
    ## 1 Overall                    28 -0.83 [-1.11; -0.55] <0.001  70.1 [56.08; 7… [-1.89; 0.23]  6.54
    ## 2 Combined                   13 -0.76 [-1.21; -0.31] 0.003   77.3 [61.5; 86… [-2.06; 0.53]  6.86
    ## 3 One ES/study (lowest)      13 -0.95 [-1.52; -0.38] 0.003   82.1 [70.54; 8… [-2.73; 0.82]  6.11
    ## 4 One ES/study (highest)     13 -0.64 [-1.02; -0.26] 0.003   69.5 [45.92; 8… [-1.61; 0.32]  7.64
    ## 5 Outliers removed           23 -0.74 [-0.86; -0.61] <0.001   0   [0; 45.37] [-0.88; -0.6]  6.99
    ## 6 Influence Analysis         25 -0.64 [-0.8; -0.49]  <0.001  38.8 [0.97; 62… [-1.18; -0.1…  7.61
    ## 7 Only rob > 2               10 -0.58 [-0.86; -0.3]  0.001   64.6 [30.37; 8… [-1.37; 0.21]  8.17
    ## 8 Three-Level Model          28 -0.84 [-1.29; -0.38] 0.002   83.4 -          [-2.35; 0.67]  6.53
    ## 9 Three-Level Model (CHE)    28 -0.82 [-1.23; -0.4]  0.001   80.2 -          [-2.18; 0.54]  6.6 
    ## 
    ## Variance components (three-level model) ---------------------- 
    ##                   tau2   i2
    ## Between Studies 0.4985 83.4
    ## Within Studies  0.0000  0.0
    ## Total           0.4985 83.4
    ## 
    ## Variance components (three-level CHE model) ------------------ 
    ##                   tau2   i2
    ## Between Studies 0.3368 67.1
    ## Within Studies  0.0656 13.1
    ## Total           0.4024 80.2

Using the `summary` method, details of the analysis settings can be printed. This function also returns recommended citations for the applied methods and/or packages.

```{r, eval=F}
summary(res)
```

    ## Analysis settings ---------------------------------------------------------- 
    ## 
    ## ✓ [Overall] Random effects model assumed. 
    ## ✓ [Overall] Heterogeneity variance (tau2) calculated using restricted maximum-likelihood estimator. 
    ## ✓ [Overall] Test statistic and CI of the pooled effect calculated using the Knapp-Hartung adjustment. 
    ## ✓ [Outliers removed] ES removed as outliers if the CI did not overlap with pooled effect CI. 
    ## ✓ [Influence Analysis] Influential cases determined using diagnostics of Viechtbauer and Cheung (2010). 
    ## [...]
    ## 
    ## 
    ## Cite the following packages/methods: ---------------------------------------
    ## 
    ##  - {meta}: Balduzzi S, Rücker G, Schwarzer G (2019),
    ##           How to perform a meta-analysis with R: a practical tutorial,
    ##           Evidence-Based Mental Health; 22: 153-160. 
    ##  - {metafor}: Viechtbauer, W. (2010). Conducting meta-analyses in R
    ##           with the metafor package. Journal of Statistical Software, 36(3), 1-48.
    ##           https://doi.org/10.18637/jss.v036.i03. 
    ##  [...]

The `runMetaAnalysis` function allows to tweak many, many details of the specific meta-analysis models (run `?runMetaAnalysis` to see the entire documentation). The most important arguments one may want to specify are:

-   `es.measure`. By default, the function conducts analyses using the Hedges $g$ values stored in the data set. To conduct analyses using dichotomous outcome data (i.e. response, remission, etc.), one has to set this argument to `"RR"`.
-   `es.type`. By default, analyses are conducted using the `"precalculated"` effect sizes stored in the prepared data set. For risk ratios, one can alternatively set this argument to `"raw"`, which means that the "raw" event counts are used for computations (instead of the pre-calculated log-risk ratio and its standard error). This is typically preferable, but may not be possible if this information could not be extracted from all primary studies.
-   `method.tau`. This argument controls the method to be used for estimating the between-study heterogeneity variance $\tau^2$. The default is `"REML"` (restricted maximum likelihood), but other options such as the DerSimonian-Laird estimator (`"DL"`) are also available (see the `runMetaAnalysis` function documentation for more details). Note that three-level meta-analysis models can only be fitted using (restricted) maximum likelihood.
-   `nnt.cer`. The `runMetaAnalysis` function uses the method by Furukawa and Leucht to calculate the Number Needed to Treat (NNT) for each pooled effect size. This method needs an estimate of the control group event rate (CER). By default, `nnt.cer = 0.2` is used, but you can set this to another value if desired.
-   `rho.within.study`. To combine effect sizes on a study level before pooling (`Combined` model), one has to assume a within-study correlation of effects. By default, `rho.within.study = 0.5` is assumed, but this value can and should be changed based on better approximations. This value also controls the assumed within-study correlation in the multilevel CHE mdoel.
-   `low.rob.filter`. By default, the function uses all comparisons for which the risk of bias rating in the `rob` variable has a value greater `2` (`low.rob.filter = "rob > 2"`). If your risk of bias rating is in another variable, or if another threshold should be used, you can change this argument accordingly.

Here is an example of a `runMetaAnalysis` call with non-default settings:

```{r, eval=FALSE}
runMetaAnalysis(data,
                es.measure = "RR",
                es.type = "raw",
                method.tau = "DL",
                nnt.cer = 0.4,
                rho.within.study = 0.8)
```

It is also possible to directly extract each calculated model from the `runMetaAnalysis` results object. Each of these models are identical to the ones one would receive by running [`metagen`](https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/pooling-es.html#pre-calculated-es) or [`rma.mv`](https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/multilevel-ma.html#multilevel-R) directly.

```{r, eval=FALSE}
res$model.overall
res$model.combined
res$model.lowest
res$model.highest
res$model.outliers
res$model.influence
res$model.rob
res$model.threelevel
res$model.threelevel.che
```

This means that **any kind of operation** available for `metagen` or `rma` models **is also available for the models created by `runMetaAnalysis`**. For example, we can generate a funnel plot for our "overall" model like this:

```{r, eval=FALSE}
library(meta)
funnel(res$model.overall, 
       studlab = TRUE, 
       contour = c(0.9, 0.95, 0.99),
       col.contour = c("darkgreen", "green", "lightgreen"))
```

<center>

![](funnel.png){width="400px"}

</center>

<br>

It is also possible to generate **forest plots** of all the calculated models. We only have to plug the results object into `plot`, and specify the name of the model for which the forest plot should be retrieved:

```{r, eval=F}
plot(res, "overall")        # Overall model (all ES assumed to be independent)
plot(res, "combined")       # ES combined within studies before pooling
plot(res, "lowest.highest") # Lowest and highest ES removed (creates 2 plots)
plot(res, "outliers")       # Outliers-removed model
plot(res, "influence")      # Influential cases-removed model
plot(res, "threelevel")     # Three-level model
plot(res, "threelevel.che") # Three-level CHE model
```

This is what the "overall" model forest plot looks like, for example:

```{r, eval = F}
plot(res, "overall")
```

<center>

![](forest.png){width="400px"}

</center>

Note that the `plot` function is simply a wrapper for the `forest.meta` function in the `meta` package. Therefore, all the advanced [styling options](https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/forest.html) are also available using extra arguments.

```{r, eval = F}
plot(res, "overall", 
     col.predict = "lightgreen", 
     col.square = "lightblue",
     fontfamily = "Palatino")
```

<center>

![](forest2.png){width="400px"}

</center>

It is also possible to generate **Baujat** and **Leave-One-Out Plots** (not displayed here).

```{r, eval = F}
plot(res, "baujat")
plot(res, "loo-es")
plot(res, "loo-i2")
```

<br>

### Publication Bias

It is also possible to correct effect size estimates for potential publication bias and/or small-study effects using the [`correctPublicationBias`](/reference/correctPublicationBias) function. This will apply three correction methods at the same time, providing a range of (more or less) plausible corrected values. Using the `which.run` argument, we can apply these methods to the `"combined"` analysis.

```{r, eval=FALSE}
correctPublicationBias(res,
                       which.run = "combined")
```

    ## Model results ------------------------------------------------ 
    ## [...]
    ## 
    ## Publication bias correction ('overall' model) ----------------------- 
    ##   model                    k     g g.ci           p        i2 i2.ci          prediction.ci   nnt
    ## 1 Trim-and-fill method    17 -0.48 [-1.04; 0.08]  0.090  82.8 [73.54; 88.75] [-2.52; 1.56]  9.4 
    ## 2 Limit meta-analysis     13  0.12 [-0.48; 0.72]  0.694  96.0 -              [-1.27; 1.51] 28.4 
    ## 3 Selection model         13 -0.72 [-1.26; -0.19] 0.008  82.2 -              [-1.89; 0.44]  7.08
    ## 
    ## Variance components (three-level model) ---------------------- 
    ## [...]

<br>

### Subgroup Analysis

The `subgroupAnalysis` function can be used to perform subgroup analyses. Every column included in the data set initially supplied to `runMetaAnalysis` can be used as a subgroup variable.

For example, we might want to check if effects differ by country (`country`) or intervention type (`condition_arm1`):

```{r, eval=F}
sg <- subgroupAnalysis(res, country, arm_format_trt1)
sg
```

    ## Subgroup analysis results ---------------------- 
    ##   variable       group n.comp     g g.ci              i2 i2.ci        nnt   p    
    ## 1 country        3          8 -1.12 [-2.33; 0.1]    86.9 [76.3; 92.7] 5.71  0.176
    ## 2 country        7          2 -0.63 [-1.64; 0.38]    0   -            7.73  0.176
    ## 3 country        1         18 -0.85 [-1.08; -0.63]  33.7 [0; 62.5]    6.47  0.176
    ## 4 condition_arm1 cbt       19 -0.81 [-1.18; -0.43]  68.1 [48.7; 80.2] 6.64  0.773
    ## 5 condition_arm1 pst        9 -0.89 [-1.39; -0.39]  75.9 [53.7; 87.5] 6.32  0.773

By default, an HTML table should also pop up. The `p` column to the right represents the significance of the overall subgroup effect (there is, for example, no significant moderator effect of `country`).

It is also possible to conduct subgroup analyses using another model, say, the three-level model. We only have to specify `.which.run`:

```{r, eval=F}
sg <- subgroupAnalysis(res, 
                       country, condition_arm1,
                       .which.run = "combined")
```

When the number of studies in subgroups are small, it is sensible to use a **common estimate** of the between-study heterogeneity variance $\tau^2$ (in lieu of subgroup-specific estimates). This can be done by setting the `.tau.common` argument to `TRUE` in the function:

```{r, eval=FALSE}
sg <- subgroupAnalysis(res, 
                       country, condition_arm1,
                       .which.run = "combined",
                       .tau.common = TRUE)
```

<br>

### Meta-Regression

Since the `runMetaAnalysis` function saves all fitted models internally, it is also possible to extract each of them individually to do further computations. Say, for example, that we want to do a multiple meta-regression using our `threelevel` model. We want to use the risk of bias rating, as well as the scaled study year as predictors. This can be achieved by extracting the model to be analyzed using the `$` operator, and then using `metaRegression` to fit the model.

```{r, eval=F}
metaRegression(res$model.threelevel, 
               ~ scale(as.numeric(rob)) + scale(year))
```

    Multivariate Meta-Analysis Model (k = 25; method: REML)

    Variance Components:

                estim    sqrt  nlvls  fixed       factor 
    sigma^2.1  0.4118  0.6417     14     no        study 
    sigma^2.2  0.0845  0.2906     25     no  study/es.id 

    Test for Residual Heterogeneity:
    QE(df = 22) = 85.9318, p-val < .0001

    Test of Moderators (coefficients 2:3):
    F(df1 = 2, df2 = 22) = 1.6824, p-val = 0.2090

    Model Results:

                            estimate      se     tval  df    pval    ci.lb    ci.ub 
    intrcpt                  -0.8063  0.1996  -4.0389  22  0.0005  -1.2203  -0.3923  *** 
    scale(as.numeric(rob))    0.4249  0.2805   1.5148  22  0.1441  -0.1568   1.0066      
    scale(year)              -0.0990  0.2965  -0.3338  22  0.7417  -0.7138   0.5159      

    ---
    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

<br></br>

<br></br>

## Study Tables

------------------------------------------------------------------------

The `createStudyTable` function allows to create an overview table for the included studies/comparisons. One only has to supply the filtered data set first, and then the names of the desired variables in the order in which they should appear in the table. It is also possible to rename factor labels directly in the function, determine how far values should be rounded, and if variable names should be changed:

```{r, eval=F}
createStudyTable(ma.data,
                 
                 ## COLUMNS --------------------------------------
                 # Simply add columns in the order in which
                 # they should appear in the table
                 study, age_group, mean_age_trt1, percent_women_trt1,
                 
                 # You can directly recode values within a variable
                 arm_format_trt1 = c("CBT" = "cbt", 
                                     "PST" = "pst",
                                     "BA" = "bat"),
                 arm_format_trt2 = c("Wait-List" = "wl", 
                                     "Care-As-Usual" = "cau"),
                 n_sessions_trt1, Post_N_trt1, Post_N_trt2, 
                 country = c("Europe" = "3", "USA" = "1",
                             "Asia" = "6", "Middle East" = "7", 
                             "Australia" = "5"),
                 sg, ac, ba, itt,
                 
                 
                 ## SPECIFICATIONS -------------------------------
                 # .round.by.digits controls the number of rounded digits for
                 # specified columns
                 .round.by.digits = list(mean_age_trt1 = 0, 
                                         Post_N_trt1 = 0,
                                         Post_N_trt2 = 0),
                 
                 # .column.names allows to rename columns
                 .column.names = list(age_group = "age group",
                                      mean_age_trt1 = "mean age",
                                      percent_women_trt1 = "% female",
                                      arm_format_trt1 = "Intervention",
                                      arm_format_trt2 = "Control",
                                      n_sessions_trt1 = "Sessions",
                                      Post_N_trt1 = "N_ig", 
                                      Post_N_trt2 = "N_cg"))
```

    ## - Creating HTML table...
    ## - [OK] Study table created successfully
    ## 
    ##                   study age group mean age % female Intervention       Control Sessions ...
    ## 1         Aagaard, 2017         4       48     0.71          CBT Care-As-Usual        8 ...
    ## 2  Allart van Dam, 2003         4       46     0.62          CBT Care-As-Usual       12 ...
    ## 3           Arean, 1993         5       66     0.75          PST     Wait-List       12 ...
    ## 4            Ayen, 2004         4       51     1.00          CBT     Wait-List       12 ...
    ## 5         Barrera, 1979         4       36     0.50           BA     Wait-List        8 ...
    ## 6           Beach, 1992         4       39     1.00          CBT     Wait-List       18 ...
    ## 7      Boeschoten, 2017         4       49     0.80          PST     Wait-List        5 ...
    ## 8          Bowman, 1995         4       36     0.63          CBT     Wait-List        4 ...
    ## 9          Bowman, 1995         4       36     0.63          PST     Wait-List        4 ...
    ## 10          Brown, 1984         4       36     0.70          CBT     Wait-List       12 ...
    ## 11          Brown, 1984         4       36     0.70          CBT     Wait-List       12 ...
    ## 12          Brown, 1984         4       36     0.70          CBT     Wait-List       12 ...
    ## 13          Carta, 2012         4       42     0.66          CBT Care-As-Usual       12 ...
    ## 14        Casanas, 2012         4       53     0.89          CBT Care-As-Usual        9 ...
    ## 15        Casanas, 2012         4       53     0.89          CBT Care-As-Usual        9 ...
    ## 16     Castonguay, 2004         4       39     0.75          CBT     Wait-List       16 ...
    ## 17     Castonguay, 2004         4       39     0.75          CBT     Wait-List       16 ...
    ## 18            Cho, 2008         4       30     1.00          CBT Care-As-Usual        9 ...
    ## 19            Cho, 2008         4       30     1.00          CBT Care-As-Usual        9 ...
    ## 20           Choi, 2012         4       39     0.80          CBT     Wait-List        6 ...
    ## 21      Chowdhary, 2016         4       41     0.69           BA Care-As-Usual        7 ...
    ## 22      Chowdhary, 2016         4       41     0.69           BA Care-As-Usual        7 ...

By default, `createStudyTable` also returns an HTML table that one can copy & paste in MS Word.

<br>

::: {style="border: 1px solid #dee2e6; background-color: #f1f3f5 !important; padding: 10px; border-radius: 3px;"}
<strong>Still have questions?</strong> This vignette only provides a superficial overview of `metapsyTools`' functionality. Every function also comes with a detailed documentation, which you may consult to learn more about available options and covered use cases. Please note that `metapsyTools` is still at an early development stage, which means that errors or other problems may still occur under some circumstances. To report an issue or ask for help, you can contact **Mathias** ([mathias.harrer\@tum.de](mailto:mathias.harrer@tum.de){.email}).
:::

<br>
